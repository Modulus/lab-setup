{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Publishing back to Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Continuing from last notebook\n",
    "\n",
    "We will revisit the previous exercise, but this time we will be publishing back results to Kafka. The idea is that we will tranform the streaming tweets into something we can use in other parts of the system. In this specific example we will filter tweets by having a specifi word, and publish only those back to another topic.\n",
    "\n",
    "Another application in the system could then subscribe to the new topic and use it for example for showing latest tweet messages in a web page.\n",
    "\n",
    "Spark streaming requires at least one [output operation](http://spark.apache.org/docs/2.1.0/streaming-programming-guide.html#output-operations-on-dstreams) in order to work. We used print judiciously in the last notebook, but will be using [foreachRDD](http://spark.apache.org/docs/2.1.0/streaming-programming-guide.html#design-patterns-for-using-foreachrdd) in order to send data out to Kafka. This requires a couple of functions to be defined beforehand. We will walk through them further down the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /home/jovyan/spark-streaming-kafka-0-8-assembly_2.11-2.1.0.jar pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import dependencies\n",
    "In addition to the previously used modules, we will install and import the kafka module. The command below will download it from the python package index repository and install it in our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!pip install kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "import json\n",
    "from kafka import KafkaProducer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Spark context\n",
    "Let's start the Spark context in local mode using all the available cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Streaming Context\n",
    "\n",
    "Let's create the streaming contaxt as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batchInterval = 10\n",
    "ssc = StreamingContext(sc, batchInterval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Checkpointing is still required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ssc.checkpoint('/tmp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Connect to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "kafka_host = 'kafka:9092'\n",
    "topic = 'Twitter.live'\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc, [topic], {'bootstrap.servers': kafka_host})\n",
    "tweets = kafkaStream.map(lambda kv: json.loads(kv[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Filter tweets by word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_filtered = tweets.filter(lambda tweet: 'iot' in tweet['text'].lower())\n",
    "tweets_filtered = tweets_filtered.map(lambda tweet: tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Lazily instantiated Kafka connection\n",
    "`foreachRDD` will be executed at the [driver](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html), and once for each RDD sent in. In order to reuse connections to the Kafka broker we will have a function that creates the `KafkaProducer` instance if it does not exist, registering it as a global variable. Subsequent calls to the function will return the already instanciated Producer and therefore avoid re-crerating it for each RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def getKafkaProducerInstance(kafka_host):\n",
    "    # lazily instantiated Kafka Producer instance\n",
    "    if ('kafkaProducerSingletonInstance' not in globals()):\n",
    "        globals()['kafkaProducerSingletonInstance'] = KafkaProducer(bootstrap_servers=kafka_host, value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "    return globals()['kafkaProducerSingletonInstance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The output function\n",
    "The `process` function will be executed each time a batch is ready, in our case it is every batchInterval. We'll collect the RDD data, fetch the Kafka Producer, and publish a new message to the broker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process(rdd_time, rdd):\n",
    "    print(\"========= %s =========\" % str(rdd_time))\n",
    "\n",
    "    try:\n",
    "        data = rdd.collect()\n",
    "        producer = getKafkaProducerInstance(globals()['kafka_host'])\n",
    "        \n",
    "        for tweet in data:\n",
    "            message = {\n",
    "                    'time': rdd_time.isoformat(),\n",
    "                    'tweet': tweet,\n",
    "                    'filter': 'iot',\n",
    "                }\n",
    "            producer.send('Twitter.processed', message)\n",
    "            print('Published: {}'.format(tweet))\n",
    "            \n",
    "        producer.flush()\n",
    "\n",
    "    except:\n",
    "        logger.exception(\"A problem has occured \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tweets_filtered.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Start the streaming context\n",
    "\n",
    "Having defined the streaming context, now we're ready to actually start it! When you run this cell, the program will start, and you'll see the result of all the `pprint` functions above appear in the output to this cell below. If you're running it outside of Jupyter (via `spark-submit`) then you'll see the output on stdout.\n",
    "\n",
    "The `timeout` will deliberately cancel the execution after two minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ssc.start()\n",
    "ssc.awaitTermination(timeout=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
